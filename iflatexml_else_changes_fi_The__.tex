\iflatexml\else \changes{ \fi
The scalar divergence can be directly adapted to SPD matrices as:
\iflatexml\else } \fi
\begin{equation}
\divB{f}(\P_1,\P_2) = \varphi(\P_1) - \varphi(\P_2) - \varphi'(\P_2)(\P_1 - \P_2),
\end{equation}
where the seed function $f$ is combined with a function $g: \GenMa \rightarrow \Re^\dc$ that maps an SPD matrix to a vector containing its eigenvalues: $\varphi = f \circ g$.\\
Or, $g$ can be the trace function, $g: \GenMa \rightarrow \Re$ that maps an SPD matrix to its trace.
For convenience, $f \circ g$ will be referred to as $f(X)$ or $f(\P)$. 

\iflatexml\else \changes{ \fi
Depending on the seed function used, various divergences can be defined defined from the Bregman divergence.
\iflatexml\else } \fi

%- Squared Norm -> Euclidean divergence = Frobenious Norm (Dhillon 2007)
%\\ \\ 
\textbf{Euclidean divergence} \\
The Frobenius norm is also a Bregman divergence in disguise. 
It is obtained when the seed function  is the squared norm $f(x) = \frac{1}{2} \lVert x \rVert_2^2$ \cite{dhillon_matrix_2007}:
\begin{equation}
\divB{E}(\P_1, \P_2) = \lVert \P_1-\P_2 \rVert _F
\label{eq:div-eucl}
\end{equation}
The Euclidean mean of SPD matrices correspond to their arithmetic mean \eqref{eq:mean_arithmetic}.
%\begin{equation}
%\PmE = \frac{1}{\Nb}\sum_{\nb=1}^{\Nb} \P_\nb
%\label{eq:mean-eucl}
%\end{equation} 

%- Shannon Entropy -> KL div (Nilson 2009)
%\\ \\ 
\textbf{Kullback-Leibler divergence} \\
Using the \emph{Shannon entropy} $f(x) = \sum_\nb x_\nb \log x_\nb$ yields the \emph{Kullback-Leibler} divergence \cite{nielsen_sided_2009}.
It is also known as the \emph{relative entropy} or \emph{discrimination information}. The Kullback-Leibler divergence of SPD matrices $\P_1, \P_2 \in \Ma$ is given by:
\begin{equation}
\divB{KL}(\P_1, \P_2) = \frac{1}{2} \log \frac{\det(\P_2)}{\det(\P_1)} + \tr(\P_2 \P_1) - \dc
\label{eq:div-kl}
\end{equation}
The mean of SPD matrices induced by the Kullback-Leibler divergence is calculated iteratively \cite{chebbi_means_2012}. 

%- logarithmic barrier function -> Log det divergence (Dhillon 2007)
%\\ \\ 
\textbf{Log-det divergence} \\
\iflatexml\else \changes{ \fi
Another function often used in Bregman divergences of symmetric matrices is the \emph{logarithmic barrier} \cite{chebbi_means_2012,dhillon_matrix_2007,cherian_efficient_2011,sra_positive_2016}
\[ f(x) = -log(x) \rightarrow f(\P) = -\log \det(\P) \] 
The corresponding divergence is called the \emph{log-det} divergence and is given by \cite{chebbi_means_2012}:
\iflatexml\else } \fi
\begin{equation}
\divB{ld}(\P_1,\P_2) = \left\langle \P_1, \P_2^{-1} \right\rangle - \log \det(\P_1 \P_2^{-1})-\dc
\label{eq:div-log-det}
\end{equation}

\iflatexml\else \changes{ \fi
The asymmetry of divergences results in the concept of right- and left-sided mean:
\[ \divB{f}(\P_1,\P_2) \neq \divB{f}(\P_2,\P_1) \Rightarrow \argmin_{\P \in \Ma} \sum_{\nb=1}^{\Nb} \dist^2(\P_\nb,\P) \neq \argmin_{\P \in \Ma} \sum_{\nb=1}^{\Nb} \dist^2(\P, \P_\nb) \]
It is usually sufficient to consider a single sided divergence. 
In this work right-sided divergence and mean are used.
In some cases however, the asymmetry can be undesirable. This has led to the symmetrization of some Bregman divergences.
Often the symmetrization consists in an averaging of left- and right-sided divergence \cite{cherian_efficient_2011}.
\iflatexml\else } \fi

%\\ \\ 
\textbf{S-divergence}\\
An example of a symmetric divergence is the S-divergence. 
It is obtained from the \emph{Jensen-Shannon} divergence which is a symmetrized Bregman divergence:
\begin{equation}
\begin{split}
\divB{J-S}(\P_1, \P_2) & = \frac{1}{2} \left( \divB{f}(\P_1,\frac{\P_1+\P_2}{2}) + \divB{f}(\frac{\P_1+\P_2}{2},\P_2) \right) \\
 & = \frac{1}{2} \left( \tr f(\P_1) + \tr f(\P_2) \right) - \tr f(\frac{\P_1+\P_2}{2})
\end{split} 
\label{eq:div-js}
\end{equation}
The S-divergence is obtained by using the logarithmic barrier function for the positive definite cone $f(\P)=-\log \det(\P)$ as seed in $\divB{S-J}$ \cite{sra_positive_2016}:
\begin{equation}
\divB{S}(\P_1,\P_2) = \log \det(\frac{\P_1+\P_2}{2}) - \frac{1}{2}\log \det(\P_1 \P_2)
\label{eq:div-s}
\end{equation}  
\iflatexml\else \changes{ \fi
Despite its symmetry, S-Divergence is not a metric. It does not satisfy the triangular inequality criterion. 
However, its square root has been shown to be a distance \cite{sra_positive_2016}.
\iflatexml\else } \fi

\iflatexml\else \changes{ \fi
Other symmetric divergences can be obtained in the same fashion; for instance the \emph{Jeffreys divergence} which is a symmetrized Kullback-Leibler divergence: $\divB{J}(\P_1, \P_2) = \divB{KL}(\P_1, \P_2) + \divB{KL}(\P_2, \P_1)$ \cite{sra_positive_2016}.
\iflatexml\else } \fi

\iflatexml\else \changes{ \fi
Another family of divergence is defined when the right- and left-sided divergence are mixed in a weighted manner.
One such family is the \emph{$\alpha$-divergence} \cite{nielsen_clustering_2014}.
\iflatexml\else } \fi

%\\ \\ 
\textbf{Log-det $\alpha$-divergence}\\
In this work, the $\alpha$-divergence used is defined by \cite{chebbi_means_2012}:
\begin{equation}
\divB{f}^\alpha(\P_1,\P_2) = \frac{4}{1-\alpha^2}\left[ \frac{1-\alpha}{2} f(\P_1) + \frac{1+\alpha}{2}f(\P_2) - f \left( \frac{1-\alpha}{2}\P_1 + \frac{1+\alpha}{2}\P_2 \right) \right], \alpha^2 \neq 1
\label{eq:div-alpha}
\end{equation}
$\divB{f}^\alpha$ can be expressed in terms of Bregman divergence as:
\begin{equation}
\divB{f}^\alpha = \frac{4}{1-\alpha^2}\left[\frac{1-\alpha}{2}\divB{f}\left(\P_1, \frac{1-\alpha}{2}\P_1+\frac{1+\alpha}{2}\P_2\right) + \frac{1+\alpha}{2}\divB{f}\left(\P_2, \frac{1-\alpha}{2}\P_1 + \frac{1+\alpha}{2} \P_2 \right)\right], \alpha^2 \neq 1
\label{eq:div-alpha2}
\end{equation}
$\alpha$-divergences at $\alpha = \pm 1$ are obtained through the limit values $\lim_{\alpha \rightarrow \pm 1}\divB{f}^\alpha$.\\
Using the logarithmic-barrier function yields:
\begin{equation}
\begin{split}
\divB{LD}^\alpha(\P_1,\P_2) &= \frac{4}{1-\alpha^2}\log \det \left(\frac{1-\alpha}{2}\left(\P_1 \P_2^{-1}\right)^{\frac{1+\alpha}{2}}+\frac{1+\alpha}{2}\left(\P_2 \P_1^{-1}\right)^{\frac{1-\alpha}{2}}\right), \quad -1 < \alpha < 1 \\
\divB{LD}^{1}(\P_1,\P_2) &= \tr \left( \P_2^{-1}\P_1-\eye \right) - \log \det \left(\P_2^{-1}\P_1 \right)\\
\divB{LD}^{-1}(\P_1,\P_2) &= \tr \left(\P_1^{-1}\P_2-\eye \right) - \log \det \left(\P_1^{-1}\P_2 \right) .
\end{split}
\label{eq:div-log-det-alpha}
\end{equation}   
$\divB{LD}^{1}$ and $\divB{LD}^{-1}$ are right- and left-sided Bregman divergences respectively.
At $\alpha=0$, the log-det $\alpha$ divergence yields a symmetric divergence corresponding to the \emph{Bhattacharrya} divergence \cite{chebbi_means_2012,sra_positive_2016}. 

%\subsubsection{log-det alpha divergence}
%\subsubsection{Kullback Leibler divergence}
%\subsubsection{Bhattacharyya}
%\subsubsection{S-divergence}

\subsubsection{Wasserstein}

The Wasserstein distance, also known as the earth mover's distance, is a measure of distance between two probability distributions. It is the optimal cost of moving one probability distribution into another. 
If the two probability distributions are pictured as two different ways of piling up a mass of sand, then the Wasserstein distance can be seen as the optimal cost of involved in transporting sand in from one piling to another~\cite{VIL08}.

Let P(\mathcal{X}) and P(\mathcal{Y}) two spaces of probability measures,
the optimal transport between two masses (or probability distributions) $\eta \in P(\mathcal{X})$ and $\nu \in P(\mathcal{Y})$ is defined as~\cite{VIL08}:
\begin{equation}
C(\eta, \nu) = \inf_{\pi \in \Pi(\eta,\nu)} \int c(x,y) d \pi(x,y),
\label{eq:opttrans}
\end{equation}
where $P(\eta, \nu)$ is the set of all joint probabilities on $\mathcal{X} \times \mathcal{Y}$; and $c(x,y)$ is the cost for transporting one unit of mass from $x$ to $y$.
In the Wassertsein distance, the cost $c(x,y)$ is defined as a distance. 
The Wassertein distance of order $p$ is defined as:
\begin{equation}
W_p(\eta,\nu) =  \left( \inf_{\pi \in \Pi(\eta,\nu)} \int d(x,y)^p d\pi(x,y) \right)^{1/p}
\label{eq:wasser}
\end{equation}
Using the Fr\'{e}chet distance, and following the development in~\cite{barbaresco_geometric_2011}, the Wasserstein distance of multivariate Gaussian measure $\Eta$ and $\Nu$ with means $\mu_X$ and $\mu_Y$, and covariance matrices $\Sigma_X$ and $\Sigma_Y$, is expressed as:
\begin{equation}
d_W(N(\mu_X, \Sigma_X),N(\mu_Y, \Sigma_Y)) = |\mu_X -\mu_Y|^2 + \tr \Sigma_X + \tr \Sigma_Y - 2 \tr \left( \left( \Sigma_X^{1/2} \Sigma_Y \Sigma_X^{1/2} \right)^{1/2} \right) \ .
\label{eq:dist_wass}
\end{equation}
Since means are null in our case, the Wasserstein distance is:
\begin{equation}
d_W(N(\mu_X, \Sigma_X),N(\mu_Y, \Sigma_Y)) = \tr \Sigma_X + \tr \Sigma_Y - 2 \tr \left( \left( \Sigma_X^{1/2} \Sigma_Y \Sigma_X^{1/2} \right)^{1/2} \right) \ .
\label{eq:dist_wass_2}
\end{equation}
%Indeed the measure can be seen as the cost of moving 

All these distances and divergences are summed up in Table~\ref{tab:dist}.

\begin{table}[h]
  \centering
  \begin{tabular}{ l | c | c | c |}
    \cline{2-4}
    & Distance/Divergence & Mean & References \rule[-5pt]{0pt}{18pt} \\ \hline
    \multicolumn{1}{|l|}{Euclidean} & $\distF(\P_1, \P_2) = \lVert \P_1-\P_2 \rVert _F$ & $\PmE = \frac{1}{\Nb}\sum_{\nb=1}^{\Nb} \P_\nb$ & \rule[-5pt]{0pt}{18pt} \\  
    \multicolumn{1}{|l|}{Harmonic} & $\distH(\P_1, \P_2) = \lVert \P_1^{-1}-\P_2^{-1} \rVert _F$ & $\PmH = \left(   \frac{1}{\Nb}\sum_{\nb=1}^{\Nb} \P_\nb^{-1} \right)^{-1} $ & \cite{lim_matrix_2012} \rule[-5pt]{0pt}{18pt} \\ 
		 \multicolumn{1}{|l|}{Log-Euclidean} & $\distLE(\P_1, \P_2) = \lVert \log(\P_1)-\log(\P_2) \rVert_F$ & $\PmLE = \exp \left( \frac{1}{\Nb} \sum_{\nb=1}^{\Nb} \log(\P_\nb) \right) $ & \cite{arsigny_geometric_2007} 
		\rule[-5pt]{0pt}{18pt} \\    
     \multicolumn{1}{|l|}{Affine-invariant} & $\distAIRM(\P_1, \P_2) = \lVert \log(\P_1^{-1}\P_2) \rVert_F$ & Algorithm 3 in \cite{fletcher_principal_2004}  & \cite{moakher_differential_2005,fletcher_principal_2004} 
		\rule[-5pt]{0pt}{18pt} \\	
	\multicolumn{1}{|l|}{Kullback-Leibler} & $\divB{KL}(\P_1, \P_2) = \frac{1}{2} \log \frac{\det(\P_2)}{\det(\P_1)} + \tr(\P_2 \P_1) - \dc$ & Algorithm 1 in \cite{chebbi_means_2012} & \cite{chebbi_means_2012,kang_composite_2009} \rule[-5pt]{0pt}{18pt} \\
	\multicolumn{1}{|l|}{S-divergence} & $\divB{S}(\P_1,\P_2) = \log \det(\frac{\P_1+\P_2}{2}) - \frac{1}{2}\log \det(\P_1 \P_2)$ &  Eq. (17-20) in \cite{cherian_efficient_2011} & \cite{sra_positive_2016,cherian_efficient_2011} \rule[-5pt]{0pt}{18pt} \\
    \multicolumn{1}{|l|}{$\alpha$-divergence} & $\divB{LD}^\alpha (\P_1, \P_2)$ from Eq.~\eqref{eq:div-log-det-alpha} & Algorithm 1 in \cite{chebbi_means_2012} & \cite{chebbi_means_2012} \rule[-5pt]{0pt}{18pt} \\ 
    \multicolumn{1}{|l|}{Bhattacharyya} & $\divB{B}(\P_1, \P_2)=\left( \log \frac{ \det \frac{1}{2} (\P_1+\P_2)}{(\det (\P_1)\det(\P_2))^{1/2}} \right)^{1/2}$ & Algorithm 1 in \cite{chebbi_means_2012} & \cite{nielsen_matrix_2012,chebbi_means_2012} \rule[-5pt]{0pt}{18pt} \\ 
    \multicolumn{1}{|l|}{Wasserstein} & $\distW = \tr \left( \P_1 + \P_2 - 2 \left( \P_1^{1/2} \P_2 \P_1^{1/2} \right)^{1/2} \right)$ & Eq. 6.2 in \cite{agueh_barycenters_2011}  & \cite{agueh_barycenters_2011, barbaresco_geometric_2011} \rule[-5pt]{0pt}{18pt} \\ \hline
  \end{tabular}
  \caption{Distances, divergences and means considered in the experimental study.}
  \label{tab:dist}
\end{table}


\subsection{Minimum Distance to Mean classifier for SSVEP}
\label{subsec:mdm}
\iflatexml\else \changes{ \fi 
The considered classifier is described in section \ref{subsec:fund_class}. 
\iflatexml\else } \fi
It is given the name \emph{Minimum Distance to Mean} or MDM, and was inspired from~\cite{barachant_multiclass_2012} where it is limited to Riemannian mean.
%The considered classifier is referred to as Minimum Distance to Mean (MDM), and is inspired from~\cite{barachant_multiclass_2012} where it is limited to Riemannian mean. 
Covariance matrices of EEG trials are classified based on their distance to the centers of the classes (i.e. means or centroids).
To embed frequency information in the covariance matrices, we use a construction of matrices proposed in~\cite{congedo_new_2013}.
%In a SSVEP experiment with $\dF$ stimulus blinking at $\dF$ different frequencies, 
Let $\X \in \Re^{\dc\times\dt}$ be an EEG trial measured on $\dc$ channels and $\dt$ samples in a SSVEP experiment with $\dF$ stimulus blinking at different frequencies.  
The covariance matrices are estimated from a modified version of the input signal $\X$: %as explained in~\cite{congedo_new_2013}
%%Let $X \in \Re^{p \times n}$ be the recorded data with $p$ the number of channels, and $n$ the number of sample per trial. 
%%The extended data are defined as:
%Let consider an experimental SSVEP setup with $\dF$ stimulus blinking at $\dF$ different frequencies. 
%It is a multiclass classification with $\dK=\dF+1$ classes: one class per stimulus and one resting state class.
%The covariance matrices are estimated from a modified version of the input signal $\X$: %as explained in~\cite{congedo_new_2013}
%Let $X \in \Re^{p \times n}$ be the recorded data with $p$ the number of channels, and $n$ the number of sample per trial. 
%The extended data are defined as:
\begin{equation}
	\X \in \Re^{\dc \times \dt} \rightarrow 	
	\begin{bmatrix}
		X_{\text{freq}_1}\\ \vdots \\ X_{\text{freq}_{\dF}} \\
	\end{bmatrix}
	\in \Re^{\dF\dc \times \dt} \ ,
	\label{eq:ext_data}
\end{equation}
where $X_{\text{freq}_\df}$ is the input signal $\X$ band-pass filtered around frequency $\text{freq}_\df$, $\df=1, \ldots, \dF$. Henceforth, all EEG signals will be considered as filtered and modified by Eq.~\eqref{eq:ext_data}.
The associated covariance matrix $\P \in \Re^{\dF\dc \times \dF\dc}$ is estimated using the Sch\"{a}fer skrinkage estimator \cite{schafer_shrinkage_2005} which was experimentally found to be the most adequate for the set of data used \cite{kalunga_online_2016}.

For SSVEP classification, $\dK = \dF + 1$ classes are considered: one class for each target frequency, and one for the resting state.
As described in Algorithm~\ref{alg:mdm}, from $\dT$ labelled training trials $ \left\{ \X_{\ti} \right\}_{\ti=1}^{\dT}$ recorded per subject, $\dK$ centers of classes $\Pm^{(\ci)}$ are estimated (step~\ref{op:class_center}). 
In this step, outliers matrices are removed to have a reliable mean estimation \cite{barachant2013riemannian}.
A new unlabeled test trial $\Y$ is predicted to belong to the class whose mean $\Pm^{(\ci)}$ is the closest to the trial covariance matrix, w.r.t. one of the distances from Table~\ref{tab:dist} (step~\ref{op:decision}).

\begin{algorithm}
\caption{Minimum Distance to Mean Classifier}
\label{alg:mdm}
	Inputs: $\X_{\ti} \in \Re^{\dF \dc\times\dt}$, for $\ti = 1, \ldots, \dT$, a set of labelled EEG trials. \\
	Inputs: $\setindex(\ci)$, a set of indices of trials belonging to class $\ci$. \\
	Input: $\Y \in \Re^{\dF \dc\times\dt}$, an unlabeled test EEG trial. \\
	Output: $\clout$, the predicted label of $\Y$.
	\begin{algorithmic}[1]
	\State Compute covariance matrices $\P_{\ti}$ of $\X_\ti$ 
	\State \textbf{for} $\ci$ = 1 \textbf{to} $\dK$ \textbf{do}
	\State \quad Compute center of class : $\Pm^{(\ci)}=\Rm(\P_{\ti}:\ti \in \setindex(\ci))$
	\label{op:class_center}
	\State \textbf{end}
	\State Compute covariance matrix $\P$ of $\Y$, and classify it : $\clout = \arg \min_{\ci} \dist(\P, \Pm^{(\ci)})$
	\label{op:decision}
	\State \textbf{return} $\clout$
	\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


